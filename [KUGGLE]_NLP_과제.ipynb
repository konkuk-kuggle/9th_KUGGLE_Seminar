{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f57TfABUH5VQ"
      },
      "source": [
        "# [KUGGLE] NLP 자연어 처리 과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohni1ZWtH5VT"
      },
      "source": [
        "########## ⭐️ (과제) 문장을 넣어주세요 ############\n",
        "\n",
        "이 부분에 자연어 처리로 분석해보고 싶은 문장을 넣어 코드를 돌려주세요!\n",
        "\n",
        "결과값과 함께 깃허브에 풀리퀘로 제출해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz0kzXJ-H5VW",
        "outputId": "e3fe8b47-ac60-4f61-e2cb-2acdf973bfc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMzSwB1lH5VT"
      },
      "source": [
        "# 1. 텍스트 토큰화 실습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMjl2TeYH5VT",
        "outputId": "bd1bc8ad-dab2-4da8-b9ef-2f357eeb0358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'안녕하세요.': 1, '쿠글': 1, '10기': 1, '서현규입니다.': 1, '파이썬으로': 1, '텍스트를': 1, '토큰화해보세요.': 1, '좋은': 1, '하루': 1, '보내세요!': 1})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def tokenize_text(text):\n",
        "    # 텍스트를 공백 기준으로 토큰화\n",
        "    tokens = text.split()\n",
        "    # 각 토큰의 빈도수 계산\n",
        "    token_counts = Counter(tokens)\n",
        "    return token_counts\n",
        "\n",
        "# 예제 문장\n",
        "example_text = \"안녕하세요. 쿠글 10기 서현규입니다. 파이썬으로 텍스트를 토큰화해보세요. 좋은 하루 보내세요!\"\n",
        "print(tokenize_text(example_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38bRzAUJLXPM"
      },
      "source": [
        "#단어집합(vocabulary) 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSH2S5r_I7gA",
        "outputId": "af33d377-707d-4f72-87d2-6dab7bde0154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어집합: ['10기', '보내세요!', '서현규입니다.', '안녕하세요.', '좋은', '쿠글', '텍스트를', '토큰화해보세요.', '파이썬으로', '하루']\n"
          ]
        }
      ],
      "source": [
        "def create_vocabulary(text):\n",
        "    # 문장을 공백을 기준으로 분리하여 단어 집합 생성\n",
        "    words = text.split()\n",
        "    # 중복 제거를 위해 집합으로 변환 후 다시 리스트로 변환\n",
        "    vocabulary = list(set(words))\n",
        "    # 단어집합을 알파벳순으로 정렬\n",
        "    vocabulary.sort()\n",
        "    return vocabulary\n",
        "\n",
        "# 예제 문장\n",
        "example_text = \"안녕하세요. 쿠글 10기 서현규입니다. 파이썬으로 텍스트를 토큰화해보세요. 좋은 하루 보내세요!\"\n",
        "\n",
        "# 단어집합 생성\n",
        "vocabulary = create_vocabulary(example_text)\n",
        "\n",
        "# 단어집합 출력\n",
        "print(\"단어집합:\", vocabulary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXtXWzUHLi5y"
      },
      "source": [
        "#정수인코딩 & 원-핫인코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4NOdp7OK7so",
        "outputId": "8a944f7e-1679-44f2-bb83-26cf154d6ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"파이썬\"의 원핫 인코딩 벡터: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def onehot_encoding(word, vocabulary):\n",
        "    # 단어의 인덱스 찾기\n",
        "    index = vocabulary.index(word)\n",
        "    # 원핫 인코딩 수행\n",
        "    onehot = np.zeros(len(vocabulary))\n",
        "    onehot[index] = 1\n",
        "    return onehot\n",
        "\n",
        "# 단어집합과 예제 단어\n",
        "vocabulary = ['안녕', '하세요', '쿠글', '10기', '서현규입니다', '파이썬', '텍스트', '토큰화해보세요', '좋은', '하루', '보내세요']\n",
        "example_word = '파이썬'\n",
        "\n",
        "# 원핫 인코딩 수행\n",
        "onehot_vector = onehot_encoding(example_word, vocabulary)\n",
        "print(f'\"{example_word}\"의 원핫 인코딩 벡터:', onehot_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KvG4l3aLyBw"
      },
      "source": [
        "#Bag-of-Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh4BxwCfL1_r",
        "outputId": "bfb3315d-8610-448d-90cb-6963e34ec15e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어집합 (vocabulary):\n",
            "안녕하세요: 1\n",
            ".: 3\n",
            "쿠글: 1\n",
            "10: 1\n",
            "기: 1\n",
            "서: 1\n",
            "현: 1\n",
            "규: 1\n",
            "입니다: 1\n",
            "파이썬: 1\n",
            "으로: 1\n",
            "텍스트: 1\n",
            "를: 1\n",
            "토큰: 1\n",
            "화: 1\n",
            "해보세요: 1\n",
            "좋은: 1\n",
            "하루: 1\n",
            "보내세요: 1\n",
            "!: 1\n",
            "\n",
            "Bag of Words (BoW) 벡터:\n",
            "[1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "def bow_representation(text):\n",
        "    # 형태소 분석기 초기화\n",
        "    okt = Okt()\n",
        "    # 텍스트를 형태소 단위로 분리\n",
        "    tokens = okt.morphs(text)\n",
        "    # 빈도수 계산\n",
        "    vocab = Counter(tokens)\n",
        "    # BoW 표현 생성\n",
        "    bow = [vocab[token] for token in tokens]\n",
        "    return vocab, bow\n",
        "\n",
        "# 예제 문장\n",
        "example_text = \"안녕하세요. 쿠글 10기 서현규입니다. 파이썬으로 텍스트를 토큰화해보세요. 좋은 하루 보내세요!\"\n",
        "\n",
        "# 단어집합과 BoW 표현 생성\n",
        "vocab, bow = bow_representation(example_text)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"단어집합 (vocabulary):\")\n",
        "for word, frequency in vocab.items():\n",
        "    print(word + \":\", frequency)\n",
        "\n",
        "print(\"\\nBag of Words (BoW) 벡터:\")\n",
        "print(bow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CauCPmhJH5VV"
      },
      "source": [
        "# 2. 형태소 분석 실습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGu2uwtOH5VW",
        "outputId": "96dd44c5-0ff7-470a-d3f9-59f69e536ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('안녕하세요', 'Adjective'), ('.', 'Punctuation'), ('쿠글', 'Noun'), ('10', 'Number'), ('기', 'Noun'), ('서', 'Modifier'), ('현', 'Modifier'), ('규', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation'), ('파이썬', 'Noun'), ('으로', 'Josa'), ('텍스트', 'Noun'), ('를', 'Josa'), ('토큰', 'Noun'), ('화', 'Suffix'), ('해보세요', 'Verb'), ('.', 'Punctuation'), ('좋은', 'Adjective'), ('하루', 'Noun'), ('보내세요', 'Verb'), ('!', 'Punctuation')]\n"
          ]
        }
      ],
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "def analyze_morphology(text):\n",
        "    okt = Okt()\n",
        "    # 형태소 분석과 품사 태깅\n",
        "    morphs = okt.pos(text)\n",
        "    return morphs\n",
        "\n",
        "# (과제) 문장 넣기\n",
        "example_text = \" 안녕하세요. 쿠글 10기 서현규입니다. 파이썬으로 텍스트를 토큰화해보세요. 좋은 하루 보내세요! \"\n",
        "print(analyze_morphology(example_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R82FFj4H5VW"
      },
      "source": [
        "# 3.N-gram 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmjHIovvH5VW",
        "outputId": "99e7bb1e-626c-4404-cea0-7deeec714ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aySPHuyfH5VX",
        "outputId": "fceb64f9-8b24-40de-88ce-637ffd683edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({('나는', '지금'): 1, ('지금', '쿠글'): 1, ('쿠글', '7주차'): 1, ('7주차', '과제를'): 1, ('과제를', '하고'): 1, ('하고', '있는'): 1, ('있는', '중이다.'): 1})\n"
          ]
        }
      ],
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def generate_ngrams(text, N=2):\n",
        "    tokens = text.split()\n",
        "    n_grams = list(ngrams(tokens, N))\n",
        "    n_gram_freq = Counter(n_grams)\n",
        "    return n_gram_freq\n",
        "\n",
        "# (과제) 문장 넣기\n",
        "example_text = \" 나는 지금 쿠글 7주차 과제를 하고 있는 중이다. \"\n",
        "print(generate_ngrams(example_text, 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kOvIVWOvzGc"
      },
      "source": [
        "# Word Embedding(GloVe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsB9DHYPuEp9",
        "outputId": "49ce2688-add0-4203-d021-2942cd212b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting glove-python3\n",
            "  Downloading glove_python3-0.1.0.tar.gz (326 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.0/327.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from glove-python3) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from glove-python3) (1.11.4)\n",
            "Building wheels for collected packages: glove-python3\n",
            "  Building wheel for glove-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python3: filename=glove_python3-0.1.0-cp310-cp310-linux_x86_64.whl size=1065515 sha256=46ee5b236c55912c67923c8b81b77bb24c948c8f804ac2d4818342499ab1cea9\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/2f/79/34314d44a0907e90e323c8c182ec23f126eb460829e02d98cf\n",
            "Successfully built glove-python3\n",
            "Installing collected packages: glove-python3\n",
            "Successfully installed glove-python3-0.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install glove-python3  #glove 라이브러리\n",
        "from glove import Corpus, Glove\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yey_lknTq1xK",
        "outputId": "62eb6ef2-f91a-4465-f31e-dceb55c3ac84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 30 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n",
            "Epoch 20\n",
            "Epoch 21\n",
            "Epoch 22\n",
            "Epoch 23\n",
            "Epoch 24\n",
            "Epoch 25\n",
            "Epoch 26\n",
            "Epoch 27\n",
            "Epoch 28\n",
            "Epoch 29\n"
          ]
        }
      ],
      "source": [
        "# 샘플 텍스트 데이터\n",
        "texts = [\n",
        "    \"GloVe is an unsupervised learning algorithm for obtaining vector representations for words.\",\n",
        "    \"Training is performed on aggregated global word-word co-occurrence statistics from a corpus.\",\n",
        "    \"The result is a set of word vectors that are interesting linear substructures of the word vector space.\",\n",
        "]\n",
        "\n",
        "# 텍스트를 토큰화합니다.\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
        "\n",
        "# Corpus 객체를 생성합니다.\n",
        "corpus = Corpus()\n",
        "\n",
        "# Corpus를 학습 데이터로 사용하여 GloVe 모델을 훈련합니다.\n",
        "corpus.fit(tokenized_texts, window=5)\n",
        "\n",
        "# GloVe 모델을 생성합니다.\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "# 사전 훈련된 Corpus를 사용하여 모델을 학습시킵니다.\n",
        "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WZxOLS7q13a",
        "outputId": "d6381f19-5056-4a8c-bc41-56a31769cc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'glove': [-4.34015957e-03  6.22384048e-04  7.16477853e-04 -4.33318190e-03\n",
            " -1.12458056e-03  1.41115546e-03  4.08675454e-03  2.89944771e-03\n",
            " -1.26135720e-03  5.03883252e-03 -3.38530366e-03  1.01761473e-03\n",
            " -1.52769508e-03  8.20448128e-04 -4.48910717e-03  1.90192416e-03\n",
            " -3.55694343e-03 -1.79046939e-03 -4.05015778e-03 -3.76207454e-05\n",
            "  5.71050694e-04  3.52100731e-03 -3.86624181e-03  1.24973247e-03\n",
            "  3.75934121e-03  2.86350130e-03  2.21432239e-03  4.08073899e-03\n",
            "  4.39390690e-03 -4.57655970e-03 -3.27325449e-03 -2.68681049e-03\n",
            "  1.46077957e-03 -1.50069155e-03 -3.32435747e-03  4.41266208e-03\n",
            " -2.47560931e-03  2.34159929e-04  1.94233923e-03 -1.98138380e-03\n",
            "  1.55789173e-03  1.78652923e-03 -4.41871427e-03 -2.10199357e-04\n",
            " -2.98133518e-03 -2.18807187e-03  1.91210322e-03 -2.62359383e-03\n",
            " -2.39301686e-03  4.00039436e-03 -4.17610337e-03  3.78834432e-03\n",
            " -2.41860197e-03 -2.86779240e-03 -3.99080952e-03 -6.40353848e-04\n",
            " -4.34513398e-04  2.70407594e-04 -3.25429012e-03  1.57694318e-03\n",
            " -1.41004176e-03 -4.06693236e-03 -2.29045939e-03 -4.07089261e-03\n",
            "  4.15715564e-03 -4.49878556e-04 -2.39575211e-03  8.51305752e-04\n",
            "  3.54245986e-03  3.19893692e-03 -1.05853015e-03 -3.73901295e-03\n",
            "  3.26334624e-03  6.13780542e-04  3.86505385e-03 -4.47071625e-03\n",
            " -2.37885815e-03  2.19160218e-03 -3.63203916e-03  4.38048792e-03\n",
            "  5.22756738e-04  1.11680354e-03 -1.62584003e-03  5.33956658e-04\n",
            " -4.15419993e-03  6.74849628e-05  4.15150308e-04  3.80127597e-03\n",
            "  8.95556231e-04  2.70686108e-03  1.01256266e-03  4.85685950e-03\n",
            "  2.31322978e-03  2.98602760e-03 -3.23159943e-03 -1.97654927e-03\n",
            "  7.15970463e-04  4.18284415e-03 -1.77156406e-03  3.30141347e-03]\n"
          ]
        }
      ],
      "source": [
        "#GloVe로 워드 임베딩한 벡터\n",
        "word = 'glove' #원하는 단어를 입력 ex) 'glove' -> '  '\n",
        "vector = glove.word_vectors[glove.dictionary[word]]\n",
        "print(f\"Vector for '{word}': {vector}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svJUhPYSyYfo"
      },
      "source": [
        "glove.most_similar()는 입력 단어의 가장 유사한 단어들의 리스트를 리턴합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg2iNH4xq195",
        "outputId": "6c19aae2-fd3d-43e6-d43b-3ede3a180493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('on', 0.2607996596120765), ('statistics', 0.22095182609452052), ('set', 0.19349708327806736), ('aggregated', 0.19117222931472896)]\n"
          ]
        }
      ],
      "source": [
        "print(glove.most_similar('glove'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG8lsSLlq2C7",
        "outputId": "08d42257-6f69-4172-ee29-b0a246d2876e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('representations', 0.2922711237841015), ('vectors', 0.2530081330911021), ('word-word', 0.17356080312114894), ('result', 0.16436078260159992)]\n"
          ]
        }
      ],
      "source": [
        "print(glove.most_similar('word'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9wKXKw-H5VX"
      },
      "source": [
        "# 4.트랜스포머를 사용한 감성 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3FY2KECH5VX",
        "outputId": "f06f0f85-6f13-4305-bb54-d9173296fe68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9GfFJLUH5VY",
        "outputId": "1e321be7-05f0-413b-8d78-19f2374317ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'NEGATIVE', 'score': 0.6522729992866516}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def sentiment_analysis(text):\n",
        "    # Hugging Face 파이프라인 사용\n",
        "    classifier = pipeline('sentiment-analysis')\n",
        "    results = classifier(text)\n",
        "    return results\n",
        "\n",
        "# (과제) 문장 넣기\n",
        "example_review = \" 나는 지금 쿠글 과제를 하고 있는 중이다. 조금 있다가 github에 제출해야 한다. \"\n",
        "print(sentiment_analysis(example_review))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}